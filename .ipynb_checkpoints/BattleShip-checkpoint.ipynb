{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78f03f72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import DQN, PPO, A2C\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from tensorflow.keras.backend import clear_session #not sure if we need this but it does not hurt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce670df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly places a ship on a board\n",
    "def set_ship(ship, ships, board, ship_locs):\n",
    "\n",
    "    grid_size = board.shape[0]\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        init_pos_i = np.random.randint(0, grid_size)\n",
    "        init_pos_j = np.random.randint(0, grid_size)\n",
    "                    \n",
    "        # for a cruiser, if init_oos_i = 0, move forward horizontally (+1)\n",
    "        # for a cruiser, if init_oos_j = 0, move downward vertically (+1)\n",
    "        move_j = grid_size - init_pos_j - ships[ship]# horizontal\n",
    "        if move_j > 0:\n",
    "            move_j = 1\n",
    "        else:\n",
    "            move_j = -1\n",
    "        move_i = grid_size - init_pos_i - ships[ship] # vertical\n",
    "        if move_i > 0:\n",
    "            move_i = 1\n",
    "        else:\n",
    "            move_i = -1\n",
    "        # choose if placing ship horizontally or vertically\n",
    "        choice_hv = np.random.choice(['h', 'v']) # horizontal, vertical\n",
    "        if choice_hv == 'h': #horizontal\n",
    "            j = [(init_pos_j + move_j*jj) for jj in range(ships[ship])]\n",
    "            i = [init_pos_i for ii in range(ships[ship])]\n",
    "            pos = set(zip(i,j))     \n",
    "            if all([board[i,j]==0 for (i,j) in pos]):\n",
    "                done = True\n",
    "        elif choice_hv == 'v':\n",
    "            i = [(init_pos_i + move_i*ii) for ii in range(ships[ship])]\n",
    "            j = [init_pos_j for jj in range(ships[ship])]\n",
    "            pos = set(zip(i,j))        \n",
    "            #check if empty board in this direction\n",
    "            if all([board[i,j]==0 for (i,j) in pos]):\n",
    "                done = True\n",
    "    # set ship - see convention\n",
    "    for (i,j) in pos:\n",
    "        board[i,j] = 1\n",
    "        ship_locs[ship].append((i,j))\n",
    "    \n",
    "    return board, ship_locs\n",
    "\n",
    "def board_rendering(grid_size, board):\n",
    "    for i in range(grid_size):\n",
    "        print(\"-\"*(4*grid_size+2))\n",
    "        for j in range(grid_size):\n",
    "            current_state_value = board[i,j]\n",
    "            current_state = ('S' if current_state_value==1 else ' ')\n",
    "            print(\" | \", end=\"\")\n",
    "            print(current_state, end='')\n",
    "        print(' |')\n",
    "    print(\"-\"*(4*grid_size+2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "755cf530",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BattleshipEnv(gym.Env):\n",
    "    \n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "    \"\"\"see https://github.com/openai/gym/blob/master/gym/core.py\"\"\"\n",
    "    \n",
    "    metadata = {'render.modes': ['human']} \n",
    "\n",
    "\n",
    "    def __init__(self, enemy_board, ship_locs, grid_size, ships):\n",
    "        \n",
    "        super(BattleshipEnv, self).__init__()\n",
    "        \n",
    "        #ships\n",
    "        self.ships = ships\n",
    "        \n",
    "        # board size\n",
    "        self.grid_size = grid_size \n",
    "        # cell state encoding (empty, hit, miss)\n",
    "        self.cell = {'E': 0, 'X': 1, 'O': -1} \n",
    "        # boards, actions, rewards\n",
    "        self.board = self.cell['E']*np.ones((self.grid_size, self.grid_size), dtype='int')\n",
    "        # enemy_board must be encoded with 0: empy and 1: ship cell\n",
    "        self.is_enemy_set = False\n",
    "        self.enemy_board = enemy_board\n",
    "        self.ship_locs = ship_locs\n",
    "        if self.enemy_board is None:\n",
    "            self.enemy_board = 0*np.ones((self.grid_size, self.grid_size), dtype='int')\n",
    "            for ship in self.ships:\n",
    "                self.ship_locs[ship] = []\n",
    "                self.enemy_board, self.ship_locs = set_ship(ship, self.ships, self.enemy_board, self.ship_locs)\n",
    "            self.is_enemy_set = True\n",
    "        # reward discount\n",
    "        self.rdisc = 0\n",
    "        self.legal_actions = [] # legal (empty) cells available for moves\n",
    "        for i in range(self.grid_size):\n",
    "            for j in range(self.grid_size):\n",
    "                self.legal_actions.append((i,j))# this gets updated as an action is performed\n",
    "        \n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # In our case the action space is discrete: index of action\n",
    "        self.action_space = spaces.Discrete(self.grid_size * self.grid_size)\n",
    "        # The observation will be the state or configuration of the board\n",
    "        self.observation_space = spaces.Box(low=-1, high=1,shape=(self.grid_size, self.grid_size), \n",
    "                                            dtype=np.int)\n",
    "\n",
    "    # action will be an index in action_space if from epsilon-greedy\n",
    "    # or from model prediction\n",
    "    def step(self, action):\n",
    "                        \n",
    "        # board situation before the action\n",
    "        state = self.board.copy()        \n",
    "        empty_cnts_pre, hit_cnts_pre, miss_cnts_pre = self.board_config(state)\n",
    "        \n",
    "        # action coordinates generated or predicted by the agent in the action_space\n",
    "        i, j = np.unravel_index(action, (self.grid_size,self.grid_size))\n",
    "        \n",
    "        #print('action', action, 'coords', i, j)\n",
    "        #print('legal_actions', self.legal_actions)\n",
    "        \n",
    "        # lose 1 point for any action\n",
    "        reward = -1\n",
    "        # assign a penalty for each illegal action used instead of a legal one\n",
    "        if (i,j) not in self.legal_actions:\n",
    "            reward -= 2*self.grid_size\n",
    "            action_idx = np.random.randint(0,len(self.legal_actions))\n",
    "            \n",
    "            i,j = self.legal_actions[action_idx]                \n",
    "            action = np.ravel_multi_index((i,j), (self.grid_size,self.grid_size))\n",
    "        \n",
    "        # set new state after performing action (scoring board is updated)\n",
    "        self.set_state((i,j))\n",
    "        # update legal actions and action_space\n",
    "        self.set_legal_actions((i,j))\n",
    "\n",
    "        # new state on scoring board - this includes last action\n",
    "        next_state = self.board\n",
    "               \n",
    "        # board situation after action\n",
    "        empty_cnts_post, hit_cnts_post, miss_cnts_post = self.board_config(next_state)\n",
    "\n",
    "        # game completed?\n",
    "        done = bool(hit_cnts_post == sum(self.ships.values()))\n",
    "                    \n",
    "        # reward for a hit\n",
    "        if hit_cnts_post-hit_cnts_pre==1: \n",
    "            # Update hit counts and use it to reward\n",
    "            r_discount = 1#0.5**self.rdisc\n",
    "            rp = (self.grid_size*self.grid_size if done else self.grid_size)\n",
    "            reward += rp*r_discount\n",
    "            #print('HIT!!!')\n",
    "            \n",
    "        #if done:\n",
    "        #    print('done')\n",
    "            \n",
    "        # we discount the reward for a subsequent hit the longer it takes to score it\n",
    "        # after a hit, zero the discount \n",
    "        # don't start discounting though, if first hit hasn't happened yet\n",
    "        #if hit_cnts_post-hit_cnts_pre==1 or hit_cnts_pre==0:\n",
    "        #    self.rdisc = 0\n",
    "        #else:\n",
    "        #    self.rdisc += 1\n",
    "                    \n",
    "        reward = float(reward)\n",
    "            \n",
    "        #print('reward:', reward)\n",
    "        # store the current value of the portfolio here\n",
    "        info = {}\n",
    "\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset the state of the environment to an initial state\n",
    "        \"\"\"\n",
    "        Important: the observation must be a numpy array\n",
    "        :return: (np.array) \n",
    "        \"\"\"\n",
    "        \n",
    "        self.board = self.cell['E']*np.ones((self.grid_size, self.grid_size), dtype='int')\n",
    "        \n",
    "        self.legal_actions = [] # legal (empty) cells available for moves\n",
    "        for i in range(self.grid_size):\n",
    "            for j in range(self.grid_size):\n",
    "                self.legal_actions.append((i,j))# this gets updated as an action is performed\n",
    "               \n",
    "        # generate a random board again if it was set randomly before\n",
    "        if self.is_enemy_set:\n",
    "            self.enemy_board = 0*np.ones((self.grid_size, self.grid_size), dtype='int')\n",
    "            self.ship_locs = {}\n",
    "            for ship in self.ships:\n",
    "                self.ship_locs[ship] = []\n",
    "                self.enemy_board, self.ship_locs = set_ship(ship, self.ships, self.enemy_board, self.ship_locs)\n",
    "\n",
    "        self.rdisc = 0\n",
    "\n",
    "        return self.board\n",
    "    \n",
    "    # Render the environment to the screen\n",
    "    # board (i,j)\n",
    "    ## ------------>j\n",
    "    ## | (0,0) | (0,1) | (0,2) | |\n",
    "    ## | (1,0) | (1,1) | (1,2) | |\n",
    "    ##                           v i\n",
    "    def render(self, mode='human'):\n",
    "        for i in range(self.grid_size):\n",
    "            print(\"-\"*(4*self.grid_size+2))\n",
    "            for j in range(self.grid_size):\n",
    "                current_state_value = self.board[i,j]\n",
    "                current_state = list(self.cell.keys())[list(self.cell.values()).index(current_state_value)]\n",
    "                current_state = (current_state if current_state!='E' else ' ')\n",
    "                print(\" | \", end=\"\")\n",
    "                print(current_state, end='')\n",
    "            print(' |')\n",
    "        print(\"-\"*(4*self.grid_size+2))\n",
    "        \n",
    "    ####### HELPER FUNCTIONS ###########\n",
    "    \n",
    "    def board_config(self, state):\n",
    "        uni_states, uni_cnts = np.unique(state.ravel(), return_counts=True)\n",
    "        empty_cnts = uni_cnts[uni_states==self.cell['E']]\n",
    "        hit_cnts = uni_cnts[uni_states==self.cell['X']]\n",
    "        miss_cnts = uni_cnts[uni_states==self.cell['O']]\n",
    "        if len(empty_cnts)==0:\n",
    "            empty_cnts = 0\n",
    "        else:\n",
    "            empty_cnts = empty_cnts[0]\n",
    "        if len(hit_cnts)==0:\n",
    "            hit_cnts = 0\n",
    "        else:\n",
    "            hit_cnts = hit_cnts[0]\n",
    "        if len(miss_cnts)==0:\n",
    "            miss_cnts = 0\n",
    "        else:\n",
    "            miss_cnts = miss_cnts[0]\n",
    "        \n",
    "        return empty_cnts, hit_cnts, miss_cnts\n",
    "\n",
    "    # set board configuration and state value after player action\n",
    "    def set_state(self, action):\n",
    "        i , j = action\n",
    "        if self.enemy_board[i,j]==1:\n",
    "            self.board[i,j]=self.cell['X']\n",
    "        else:\n",
    "            self.board[i,j]=self.cell['O']\n",
    "\n",
    "    # set legal actions (empty board locations)\n",
    "    def set_legal_actions(self, action):\n",
    "        if action in self.legal_actions:\n",
    "            self.legal_actions.remove(action)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff249f2",
   "metadata": {},
   "source": [
    "## Validate environment with either random defined ship on 5x5 board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09618b48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enes1\\AppData\\Local\\Temp/ipykernel_12728/2322792659.py:45: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int)\n",
      "C:\\Users\\enes1\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\env_checker.py:190: UserWarning: Your observation  has an unconventional shape (neither an image, nor a 1D vector). We recommend you to flatten the observation to have only a 1D vector or use a custom policy to properly process the data.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ships\n",
    "ships = {}\n",
    "ships['cruiser'] = 3\n",
    "\n",
    "grid_size = 5\n",
    "# for pre-determined board\n",
    "# enemy_board = 0*np.ones((grid_size, grid_size), dtype='int')\n",
    "# enemy_board[0,1] = 1\n",
    "# enemy_board[1,1] = 1\n",
    "# enemy_board[2,1] = 1\n",
    "# ship_locs = {}\n",
    "# ship_locs['cruiser'] = [(0,1),(1,1),(2,1)]\n",
    "# env = BattleshipEnv(enemy_board=enemy_board, ship_locs=ship_locs, grid_size=grid_size, ships=ships)\n",
    "# for random board\n",
    "env = BattleshipEnv(enemy_board=None, ship_locs={}, grid_size=grid_size, ships=ships)\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af3a1c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 1, 1, 1, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0]]),\n",
       " {'cruiser': [(0, 1), (0, 2), (0, 3)]})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.enemy_board, env.ship_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d06fbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enes1\\AppData\\Local\\Temp/ipykernel_12728/2322792659.py:45: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\n",
      "Action 1 row: 0 index: 1\n",
      "obs= [[ 0 -1  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]] reward= -1.0 done= False\n",
      "----------------------\n",
      " |   | O |   |   |   |\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      "Action 2 row: 1 index: 2\n",
      "obs= [[ 0 -1  0  0  0]\n",
      " [ 0  0 -1  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]] reward= -1.0 done= False\n",
      "----------------------\n",
      " |   | O |   |   |   |\n",
      "----------------------\n",
      " |   |   | O |   |   |\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      "Action 3 row: 0 index: 3\n",
      "obs= [[ 0 -1  0  1  0]\n",
      " [ 0  0 -1  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]] reward= 4.0 done= False\n",
      "----------------------\n",
      " |   | O |   | X |   |\n",
      "----------------------\n",
      " |   |   | O |   |   |\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      "Action 4 row: 2 index: 1\n",
      "obs= [[ 0 -1  0  1  0]\n",
      " [ 0  0 -1  0  0]\n",
      " [ 0 -1  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]] reward= -1.0 done= False\n",
      "----------------------\n",
      " |   | O |   | X |   |\n",
      "----------------------\n",
      " |   |   | O |   |   |\n",
      "----------------------\n",
      " |   | O |   |   |   |\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      "Action 5 row: 1 index: 0\n",
      "obs= [[ 0 -1  0  1  0]\n",
      " [-1  0 -1  0  0]\n",
      " [ 0 -1  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]] reward= -1.0 done= False\n",
      "----------------------\n",
      " |   | O |   | X |   |\n",
      "----------------------\n",
      " | O |   | O |   |   |\n",
      "----------------------\n",
      " |   | O |   |   |   |\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      "Action 6 row: 0 index: 4\n",
      "obs= [[ 0 -1  0  1 -1]\n",
      " [-1  0 -1  0  0]\n",
      " [ 0 -1  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]] reward= -1.0 done= False\n",
      "----------------------\n",
      " |   | O |   | X | O |\n",
      "----------------------\n",
      " | O |   | O |   |   |\n",
      "----------------------\n",
      " |   | O |   |   |   |\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      "Action 7 row: 2 index: 3\n",
      "obs= [[ 0 -1  0  1 -1]\n",
      " [-1  0 -1  0  0]\n",
      " [ 0 -1  0  1  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]] reward= 4.0 done= False\n",
      "----------------------\n",
      " |   | O |   | X | O |\n",
      "----------------------\n",
      " | O |   | O |   |   |\n",
      "----------------------\n",
      " |   | O |   | X |   |\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      "Action 8 row: 1 index: 2\n",
      "obs= [[ 0 -1  0  1 -1]\n",
      " [-1  0 -1  0  0]\n",
      " [ 0 -1  0  1  0]\n",
      " [ 0  0 -1  0  0]\n",
      " [ 0  0  0  0  0]] reward= -11.0 done= False\n",
      "----------------------\n",
      " |   | O |   | X | O |\n",
      "----------------------\n",
      " | O |   | O |   |   |\n",
      "----------------------\n",
      " |   | O |   | X |   |\n",
      "----------------------\n",
      " |   |   | O |   |   |\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      "Action 9 row: 2 index: 3\n",
      "obs= [[-1 -1  0  1 -1]\n",
      " [-1  0 -1  0  0]\n",
      " [ 0 -1  0  1  0]\n",
      " [ 0  0 -1  0  0]\n",
      " [ 0  0  0  0  0]] reward= -11.0 done= False\n",
      "----------------------\n",
      " | O | O |   | X | O |\n",
      "----------------------\n",
      " | O |   | O |   |   |\n",
      "----------------------\n",
      " |   | O |   | X |   |\n",
      "----------------------\n",
      " |   |   | O |   |   |\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      "Action 10 row: 1 index: 1\n",
      "obs= [[-1 -1  0  1 -1]\n",
      " [-1 -1 -1  0  0]\n",
      " [ 0 -1  0  1  0]\n",
      " [ 0  0 -1  0  0]\n",
      " [ 0  0  0  0  0]] reward= -1.0 done= False\n",
      "----------------------\n",
      " | O | O |   | X | O |\n",
      "----------------------\n",
      " | O | O | O |   |   |\n",
      "----------------------\n",
      " |   | O |   | X |   |\n",
      "----------------------\n",
      " |   |   | O |   |   |\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      "Action 11 row: 2 index: 1\n",
      "obs= [[-1 -1  0  1 -1]\n",
      " [-1 -1 -1  0  0]\n",
      " [ 0 -1  0  1  0]\n",
      " [ 0  0 -1  0  0]\n",
      " [ 0  0  0 -1  0]] reward= -11.0 done= False\n",
      "----------------------\n",
      " | O | O |   | X | O |\n",
      "----------------------\n",
      " | O | O | O |   |   |\n",
      "----------------------\n",
      " |   | O |   | X |   |\n",
      "----------------------\n",
      " |   |   | O |   |   |\n",
      "----------------------\n",
      " |   |   |   | O |   |\n",
      "----------------------\n",
      "Action 12 row: 1 index: 2\n",
      "obs= [[-1 -1  0  1 -1]\n",
      " [-1 -1 -1  0  0]\n",
      " [ 0 -1  0  1  0]\n",
      " [ 0  0 -1  0  0]\n",
      " [-1  0  0 -1  0]] reward= -11.0 done= False\n",
      "----------------------\n",
      " | O | O |   | X | O |\n",
      "----------------------\n",
      " | O | O | O |   |   |\n",
      "----------------------\n",
      " |   | O |   | X |   |\n",
      "----------------------\n",
      " |   |   | O |   |   |\n",
      "----------------------\n",
      " | O |   |   | O |   |\n",
      "----------------------\n",
      "Action 13 row: 2 index: 2\n",
      "obs= [[-1 -1  0  1 -1]\n",
      " [-1 -1 -1  0  0]\n",
      " [ 0 -1 -1  1  0]\n",
      " [ 0  0 -1  0  0]\n",
      " [-1  0  0 -1  0]] reward= -1.0 done= False\n",
      "----------------------\n",
      " | O | O |   | X | O |\n",
      "----------------------\n",
      " | O | O | O |   |   |\n",
      "----------------------\n",
      " |   | O | O | X |   |\n",
      "----------------------\n",
      " |   |   | O |   |   |\n",
      "----------------------\n",
      " | O |   |   | O |   |\n",
      "----------------------\n",
      "Action 14 row: 4 index: 2\n",
      "obs= [[-1 -1  0  1 -1]\n",
      " [-1 -1 -1  0  0]\n",
      " [ 0 -1 -1  1  0]\n",
      " [ 0  0 -1  0  0]\n",
      " [-1  0 -1 -1  0]] reward= -1.0 done= False\n",
      "----------------------\n",
      " | O | O |   | X | O |\n",
      "----------------------\n",
      " | O | O | O |   |   |\n",
      "----------------------\n",
      " |   | O | O | X |   |\n",
      "----------------------\n",
      " |   |   | O |   |   |\n",
      "----------------------\n",
      " | O |   | O | O |   |\n",
      "----------------------\n",
      "Action 15 row: 1 index: 3\n",
      "obs= [[-1 -1  0  1 -1]\n",
      " [-1 -1 -1  1  0]\n",
      " [ 0 -1 -1  1  0]\n",
      " [ 0  0 -1  0  0]\n",
      " [-1  0 -1 -1  0]] reward= 24.0 done= True\n",
      "----------------------\n",
      " | O | O |   | X | O |\n",
      "----------------------\n",
      " | O | O | O | X |   |\n",
      "----------------------\n",
      " |   | O | O | X |   |\n",
      "----------------------\n",
      " |   |   | O |   |   |\n",
      "----------------------\n",
      " | O |   | O | O |   |\n",
      "----------------------\n",
      "Goal reached! reward= 24.0\n",
      "Episode 1\n",
      "Action 1 row: 1 index: 4\n",
      "obs= [[ 0  0  0  0  0]\n",
      " [ 0  0  0  0 -1]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]] reward= -1.0 done= False\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      " |   |   |   |   | O |\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      "Action 2 row: 3 index: 0\n",
      "obs= [[ 0  0  0  0  0]\n",
      " [ 0  0  0  0 -1]\n",
      " [ 0  0  0  0  0]\n",
      " [-1  0  0  0  0]\n",
      " [ 0  0  0  0  0]] reward= -1.0 done= False\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      " |   |   |   |   | O |\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      " | O |   |   |   |   |\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      "Action 3 row: 2 index: 3\n",
      "obs= [[ 0  0  0  0  0]\n",
      " [ 0  0  0  0 -1]\n",
      " [ 0  0  0 -1  0]\n",
      " [-1  0  0  0  0]\n",
      " [ 0  0  0  0  0]] reward= -1.0 done= False\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      " |   |   |   |   | O |\n",
      "----------------------\n",
      " |   |   |   | O |   |\n",
      "----------------------\n",
      " | O |   |   |   |   |\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      "Action 4 row: 3 index: 0\n",
      "obs= [[ 0  0  0  0  0]\n",
      " [ 0  0  0  0 -1]\n",
      " [ 0  0  0 -1  0]\n",
      " [-1  0  0 -1  0]\n",
      " [ 0  0  0  0  0]] reward= -11.0 done= False\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      " |   |   |   |   | O |\n",
      "----------------------\n",
      " |   |   |   | O |   |\n",
      "----------------------\n",
      " | O |   |   | O |   |\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      "Action 5 row: 4 index: 3\n",
      "obs= [[ 0  0  0  0  0]\n",
      " [ 0  0  0  0 -1]\n",
      " [ 0  0  0 -1  0]\n",
      " [-1  0  0 -1  0]\n",
      " [ 0  0  0 -1  0]] reward= -1.0 done= False\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      " |   |   |   |   | O |\n",
      "----------------------\n",
      " |   |   |   | O |   |\n",
      "----------------------\n",
      " | O |   |   | O |   |\n",
      "----------------------\n",
      " |   |   |   | O |   |\n",
      "----------------------\n",
      "Action 6 row: 2 index: 3\n",
      "obs= [[ 0  0  0  0  0]\n",
      " [ 0  0  0  0 -1]\n",
      " [ 0 -1  0 -1  0]\n",
      " [-1  0  0 -1  0]\n",
      " [ 0  0  0 -1  0]] reward= -11.0 done= False\n",
      "----------------------\n",
      " |   |   |   |   |   |\n",
      "----------------------\n",
      " |   |   |   |   | O |\n",
      "----------------------\n",
      " |   | O |   | O |   |\n",
      "----------------------\n",
      " | O |   |   | O |   |\n",
      "----------------------\n",
      " |   |   |   | O |   |\n",
      "----------------------\n",
      "Action 7 row: 2 index: 3\n",
      "obs= [[ 0  0 -1  0  0]\n",
      " [ 0  0  0  0 -1]\n",
      " [ 0 -1  0 -1  0]\n",
      " [-1  0  0 -1  0]\n",
      " [ 0  0  0 -1  0]] reward= -11.0 done= False\n",
      "----------------------\n",
      " |   |   | O |   |   |\n",
      "----------------------\n",
      " |   |   |   |   | O |\n",
      "----------------------\n",
      " |   | O |   | O |   |\n",
      "----------------------\n",
      " | O |   |   | O |   |\n",
      "----------------------\n",
      " |   |   |   | O |   |\n",
      "----------------------\n",
      "Action 8 row: 3 index: 0\n",
      "obs= [[ 0  0 -1  0 -1]\n",
      " [ 0  0  0  0 -1]\n",
      " [ 0 -1  0 -1  0]\n",
      " [-1  0  0 -1  0]\n",
      " [ 0  0  0 -1  0]] reward= -11.0 done= False\n",
      "----------------------\n",
      " |   |   | O |   | O |\n",
      "----------------------\n",
      " |   |   |   |   | O |\n",
      "----------------------\n",
      " |   | O |   | O |   |\n",
      "----------------------\n",
      " | O |   |   | O |   |\n",
      "----------------------\n",
      " |   |   |   | O |   |\n",
      "----------------------\n",
      "Action 9 row: 1 index: 1\n",
      "obs= [[ 0  0 -1  0 -1]\n",
      " [ 0  1  0  0 -1]\n",
      " [ 0 -1  0 -1  0]\n",
      " [-1  0  0 -1  0]\n",
      " [ 0  0  0 -1  0]] reward= 4.0 done= False\n",
      "----------------------\n",
      " |   |   | O |   | O |\n",
      "----------------------\n",
      " |   | X |   |   | O |\n",
      "----------------------\n",
      " |   | O |   | O |   |\n",
      "----------------------\n",
      " | O |   |   | O |   |\n",
      "----------------------\n",
      " |   |   |   | O |   |\n",
      "----------------------\n",
      "Action 10 row: 0 index: 3\n",
      "obs= [[ 0  0 -1 -1 -1]\n",
      " [ 0  1  0  0 -1]\n",
      " [ 0 -1  0 -1  0]\n",
      " [-1  0  0 -1  0]\n",
      " [ 0  0  0 -1  0]] reward= -1.0 done= False\n",
      "----------------------\n",
      " |   |   | O | O | O |\n",
      "----------------------\n",
      " |   | X |   |   | O |\n",
      "----------------------\n",
      " |   | O |   | O |   |\n",
      "----------------------\n",
      " | O |   |   | O |   |\n",
      "----------------------\n",
      " |   |   |   | O |   |\n",
      "----------------------\n",
      "Action 11 row: 3 index: 2\n",
      "obs= [[ 0  0 -1 -1 -1]\n",
      " [ 0  1  0  0 -1]\n",
      " [ 0 -1  0 -1  0]\n",
      " [-1  0 -1 -1  0]\n",
      " [ 0  0  0 -1  0]] reward= -1.0 done= False\n",
      "----------------------\n",
      " |   |   | O | O | O |\n",
      "----------------------\n",
      " |   | X |   |   | O |\n",
      "----------------------\n",
      " |   | O |   | O |   |\n",
      "----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | O |   | O | O |   |\n",
      "----------------------\n",
      " |   |   |   | O |   |\n",
      "----------------------\n",
      "Action 12 row: 4 index: 1\n",
      "obs= [[ 0  0 -1 -1 -1]\n",
      " [ 0  1  0  0 -1]\n",
      " [ 0 -1  0 -1  0]\n",
      " [-1  0 -1 -1  0]\n",
      " [ 0 -1  0 -1  0]] reward= -1.0 done= False\n",
      "----------------------\n",
      " |   |   | O | O | O |\n",
      "----------------------\n",
      " |   | X |   |   | O |\n",
      "----------------------\n",
      " |   | O |   | O |   |\n",
      "----------------------\n",
      " | O |   | O | O |   |\n",
      "----------------------\n",
      " |   | O |   | O |   |\n",
      "----------------------\n",
      "Action 13 row: 0 index: 1\n",
      "obs= [[ 0 -1 -1 -1 -1]\n",
      " [ 0  1  0  0 -1]\n",
      " [ 0 -1  0 -1  0]\n",
      " [-1  0 -1 -1  0]\n",
      " [ 0 -1  0 -1  0]] reward= -1.0 done= False\n",
      "----------------------\n",
      " |   | O | O | O | O |\n",
      "----------------------\n",
      " |   | X |   |   | O |\n",
      "----------------------\n",
      " |   | O |   | O |   |\n",
      "----------------------\n",
      " | O |   | O | O |   |\n",
      "----------------------\n",
      " |   | O |   | O |   |\n",
      "----------------------\n",
      "Action 14 row: 0 index: 2\n",
      "obs= [[ 0 -1 -1 -1 -1]\n",
      " [ 0  1  0  0 -1]\n",
      " [ 0 -1  0 -1  0]\n",
      " [-1  0 -1 -1 -1]\n",
      " [ 0 -1  0 -1  0]] reward= -11.0 done= False\n",
      "----------------------\n",
      " |   | O | O | O | O |\n",
      "----------------------\n",
      " |   | X |   |   | O |\n",
      "----------------------\n",
      " |   | O |   | O |   |\n",
      "----------------------\n",
      " | O |   | O | O | O |\n",
      "----------------------\n",
      " |   | O |   | O |   |\n",
      "----------------------\n",
      "Action 15 row: 2 index: 4\n",
      "obs= [[ 0 -1 -1 -1 -1]\n",
      " [ 0  1  0  0 -1]\n",
      " [ 0 -1  0 -1 -1]\n",
      " [-1  0 -1 -1 -1]\n",
      " [ 0 -1  0 -1  0]] reward= -1.0 done= False\n",
      "----------------------\n",
      " |   | O | O | O | O |\n",
      "----------------------\n",
      " |   | X |   |   | O |\n",
      "----------------------\n",
      " |   | O |   | O | O |\n",
      "----------------------\n",
      " | O |   | O | O | O |\n",
      "----------------------\n",
      " |   | O |   | O |   |\n",
      "----------------------\n",
      "Action 16 row: 3 index: 0\n",
      "obs= [[ 0 -1 -1 -1 -1]\n",
      " [ 0  1  0  0 -1]\n",
      " [ 0 -1  0 -1 -1]\n",
      " [-1  0 -1 -1 -1]\n",
      " [-1 -1  0 -1  0]] reward= -11.0 done= False\n",
      "----------------------\n",
      " |   | O | O | O | O |\n",
      "----------------------\n",
      " |   | X |   |   | O |\n",
      "----------------------\n",
      " |   | O |   | O | O |\n",
      "----------------------\n",
      " | O |   | O | O | O |\n",
      "----------------------\n",
      " | O | O |   | O |   |\n",
      "----------------------\n",
      "Action 17 row: 1 index: 4\n",
      "obs= [[ 0 -1 -1 -1 -1]\n",
      " [ 0  1  0  0 -1]\n",
      " [ 0 -1  0 -1 -1]\n",
      " [-1 -1 -1 -1 -1]\n",
      " [-1 -1  0 -1  0]] reward= -11.0 done= False\n",
      "----------------------\n",
      " |   | O | O | O | O |\n",
      "----------------------\n",
      " |   | X |   |   | O |\n",
      "----------------------\n",
      " |   | O |   | O | O |\n",
      "----------------------\n",
      " | O | O | O | O | O |\n",
      "----------------------\n",
      " | O | O |   | O |   |\n",
      "----------------------\n",
      "Action 18 row: 0 index: 3\n",
      "obs= [[-1 -1 -1 -1 -1]\n",
      " [ 0  1  0  0 -1]\n",
      " [ 0 -1  0 -1 -1]\n",
      " [-1 -1 -1 -1 -1]\n",
      " [-1 -1  0 -1  0]] reward= -11.0 done= False\n",
      "----------------------\n",
      " | O | O | O | O | O |\n",
      "----------------------\n",
      " |   | X |   |   | O |\n",
      "----------------------\n",
      " |   | O |   | O | O |\n",
      "----------------------\n",
      " | O | O | O | O | O |\n",
      "----------------------\n",
      " | O | O |   | O |   |\n",
      "----------------------\n",
      "Action 19 row: 0 index: 2\n",
      "obs= [[-1 -1 -1 -1 -1]\n",
      " [ 1  1  0  0 -1]\n",
      " [ 0 -1  0 -1 -1]\n",
      " [-1 -1 -1 -1 -1]\n",
      " [-1 -1  0 -1  0]] reward= -6.0 done= False\n",
      "----------------------\n",
      " | O | O | O | O | O |\n",
      "----------------------\n",
      " | X | X |   |   | O |\n",
      "----------------------\n",
      " |   | O |   | O | O |\n",
      "----------------------\n",
      " | O | O | O | O | O |\n",
      "----------------------\n",
      " | O | O |   | O |   |\n",
      "----------------------\n",
      "Action 20 row: 4 index: 4\n",
      "obs= [[-1 -1 -1 -1 -1]\n",
      " [ 1  1  0  0 -1]\n",
      " [ 0 -1  0 -1 -1]\n",
      " [-1 -1 -1 -1 -1]\n",
      " [-1 -1  0 -1 -1]] reward= -1.0 done= False\n",
      "----------------------\n",
      " | O | O | O | O | O |\n",
      "----------------------\n",
      " | X | X |   |   | O |\n",
      "----------------------\n",
      " |   | O |   | O | O |\n",
      "----------------------\n",
      " | O | O | O | O | O |\n",
      "----------------------\n",
      " | O | O |   | O | O |\n",
      "----------------------\n",
      "Action 21 row: 0 index: 4\n",
      "obs= [[-1 -1 -1 -1 -1]\n",
      " [ 1  1  0 -1 -1]\n",
      " [ 0 -1  0 -1 -1]\n",
      " [-1 -1 -1 -1 -1]\n",
      " [-1 -1  0 -1 -1]] reward= -11.0 done= False\n",
      "----------------------\n",
      " | O | O | O | O | O |\n",
      "----------------------\n",
      " | X | X |   | O | O |\n",
      "----------------------\n",
      " |   | O |   | O | O |\n",
      "----------------------\n",
      " | O | O | O | O | O |\n",
      "----------------------\n",
      " | O | O |   | O | O |\n",
      "----------------------\n",
      "Action 22 row: 1 index: 1\n",
      "obs= [[-1 -1 -1 -1 -1]\n",
      " [ 1  1  0 -1 -1]\n",
      " [-1 -1  0 -1 -1]\n",
      " [-1 -1 -1 -1 -1]\n",
      " [-1 -1  0 -1 -1]] reward= -11.0 done= False\n",
      "----------------------\n",
      " | O | O | O | O | O |\n",
      "----------------------\n",
      " | X | X |   | O | O |\n",
      "----------------------\n",
      " | O | O |   | O | O |\n",
      "----------------------\n",
      " | O | O | O | O | O |\n",
      "----------------------\n",
      " | O | O |   | O | O |\n",
      "----------------------\n",
      "Action 23 row: 3 index: 4\n",
      "obs= [[-1 -1 -1 -1 -1]\n",
      " [ 1  1  1 -1 -1]\n",
      " [-1 -1  0 -1 -1]\n",
      " [-1 -1 -1 -1 -1]\n",
      " [-1 -1  0 -1 -1]] reward= 14.0 done= True\n",
      "----------------------\n",
      " | O | O | O | O | O |\n",
      "----------------------\n",
      " | X | X | X | O | O |\n",
      "----------------------\n",
      " | O | O |   | O | O |\n",
      "----------------------\n",
      " | O | O | O | O | O |\n",
      "----------------------\n",
      " | O | O |   | O | O |\n",
      "----------------------\n",
      "Goal reached! reward= 14.0\n"
     ]
    }
   ],
   "source": [
    "# Test environment\n",
    "# ships\n",
    "ships = {}\n",
    "ships['cruiser'] = 3\n",
    "\n",
    "grid_size=5\n",
    "env = BattleshipEnv(enemy_board=None, ship_locs={}, grid_size=grid_size, ships=ships)\n",
    "\n",
    "for ep in range(2):\n",
    "    print('Episode', ep)\n",
    "    obs = env.reset()\n",
    "    #env.render()\n",
    "    #print(env.enemy_board)\n",
    "    done = False\n",
    "    t = 0\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        i, j = np.unravel_index(action, (grid_size,grid_size))    \n",
    "        print(\"Action {}\".format(t + 1), \"row:\", i, \"index:\", j)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        print('obs=', obs, 'reward=', reward, 'done=', done)\n",
    "        env.render()\n",
    "        t += 1\n",
    "        if done:\n",
    "            print(\"Goal reached!\", \"reward=\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6649d5",
   "metadata": {},
   "source": [
    "## Callback and Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe2b52d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, episode_interval: int, log_dir: str, verbose=1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.episode_interval = episode_interval\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model.pkl')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            # Evaluate policy training performance\n",
    "            x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "            if len(x) > 0:\n",
    "                # NOTE: when done is True, timesteps are counted and reported to the log_dir\n",
    "                mean_reward = np.mean(y[-self.episode_interval:]) # mean reward over previous episode_interval episodes\n",
    "                mean_moves = np.mean(np.diff(x[-self.episode_interval:])) # mean moves over previous 100 episodes\n",
    "                if self.verbose > 0:\n",
    "                    print(x[-1], 'timesteps') # closest to step_interval step number\n",
    "                    print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f} - Last mean moves per episode: {:.2f}\".format(self.best_mean_reward, \n",
    "                                                                                                   mean_reward, mean_moves))\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose > 0:\n",
    "                        print(\"Saving new best model\")\n",
    "                    self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "945ee372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(values, window):\n",
    "    \"\"\"\n",
    "    Smooth values by doing a moving average\n",
    "    :param values: (numpy array)\n",
    "    :param window: (int)\n",
    "    :return: (numpy array)\n",
    "    \"\"\"\n",
    "    weights = np.repeat(1.0, window) / window\n",
    "    return np.convolve(values, weights, 'valid')\n",
    "\n",
    "\n",
    "def plot_results(log_folder, window = 100, title='Learning Curve'):\n",
    "    \"\"\"\n",
    "    plot the results\n",
    "\n",
    "    :param log_folder: (str) the save location of the results to plot\n",
    "    :param title: (str) the title of the task to plot\n",
    "    \"\"\"\n",
    "    \n",
    "    x, y = ts2xy(load_results(log_folder), 'timesteps')\n",
    "    y = moving_average(y, window=window)\n",
    "    y_moves = moving_average(np.diff(x), window = window) \n",
    "    # Truncate x\n",
    "    x = x[len(x) - len(y):]\n",
    "    x_moves = x[len(x) - len(y_moves):]\n",
    "\n",
    "    title = 'Smoothed Learning Curve of Rewards (every ' + str(window) +' steps)'\n",
    "    fig = plt.figure(title)\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('Number of Timesteps')\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "    title = 'Smoothed Learning Curve of Moves (every ' + str(window) +' steps)'\n",
    "    fig = plt.figure(title)\n",
    "    plt.plot(x_moves, y_moves)\n",
    "    plt.xlabel('Number of Timesteps')\n",
    "    plt.ylabel('Moves')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9492633",
   "metadata": {},
   "source": [
    "## Making the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f2cd5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(_locals, _globals):\n",
    "    \"\"\"\n",
    "    Callback called at each step (for DQN an others) or after n steps (see ACER or PPO2)\n",
    "    :param _locals: (dict)\n",
    "    :param _globals: (dict)\n",
    "    \"\"\"\n",
    "    global n_steps, best_mean_reward\n",
    "    # Print stats every step_interval calls\n",
    "    if (n_steps + 1) % step_interval == 0:\n",
    "        # Evaluate policy training performance\n",
    "        x, y = ts2xy(load_results(log_dir), 'timesteps')\n",
    "        if len(x) > 0:\n",
    "            # NOTE: when done is True, timesteps are counted and reported to the log_dir\n",
    "            mean_reward = np.mean(y[-episode_interval:]) # mean reward over previous episode_interval episodes\n",
    "            mean_moves = np.mean(np.diff(x[-episode_interval:])) # mean moves over previous episode_interval episodes\n",
    "            print(x[-1], 'timesteps') # closest to step_interval step number\n",
    "            print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f} - Last mean moves per episode: {:.2f}\".format(best_mean_reward, \n",
    "                                                                                           mean_reward, mean_moves))\n",
    "\n",
    "            # New best model, you could save the agent here\n",
    "            if mean_reward > best_mean_reward:\n",
    "                best_mean_reward = mean_reward\n",
    "                # Example for saving best model\n",
    "                print(\"Saving new best model\")\n",
    "                _locals['self'].save(log_dir + 'best_model.pkl')\n",
    "    n_steps += 1\n",
    "    # Returning False will stop training early\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070ba9bc",
   "metadata": {},
   "source": [
    "## Trainig on 5x5 board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35bf3df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enes1\\AppData\\Local\\Temp/ipykernel_12728/2322792659.py:45: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9997 timesteps\n",
      "Best mean reward: -inf - Last mean reward per episode: -58.17 - Last mean moves per episode: 19.11\n",
      "Saving new best model\n",
      "19983 timesteps\n",
      "Best mean reward: -58.17 - Last mean reward per episode: -54.68 - Last mean moves per episode: 18.78\n",
      "Saving new best model\n",
      "29996 timesteps\n",
      "Best mean reward: -54.68 - Last mean reward per episode: -49.91 - Last mean moves per episode: 18.28\n",
      "Saving new best model\n",
      "39994 timesteps\n",
      "Best mean reward: -49.91 - Last mean reward per episode: -46.35 - Last mean moves per episode: 17.79\n",
      "Saving new best model\n",
      "49980 timesteps\n",
      "Best mean reward: -46.35 - Last mean reward per episode: -44.28 - Last mean moves per episode: 17.40\n",
      "Saving new best model\n",
      "59977 timesteps\n",
      "Best mean reward: -44.28 - Last mean reward per episode: -43.13 - Last mean moves per episode: 17.11\n",
      "Saving new best model\n",
      "69984 timesteps\n",
      "Best mean reward: -43.13 - Last mean reward per episode: -42.16 - Last mean moves per episode: 16.87\n",
      "Saving new best model\n",
      "79995 timesteps\n",
      "Best mean reward: -42.16 - Last mean reward per episode: -41.41 - Last mean moves per episode: 16.68\n",
      "Saving new best model\n",
      "89995 timesteps\n",
      "Best mean reward: -41.41 - Last mean reward per episode: -40.44 - Last mean moves per episode: 16.45\n",
      "Saving new best model\n",
      "100000 timesteps\n",
      "Best mean reward: -40.44 - Last mean reward per episode: -39.83 - Last mean moves per episode: 16.31\n",
      "Saving new best model\n"
     ]
    }
   ],
   "source": [
    "clear_session()\n",
    "\n",
    "# ships -- keep only one kind for 5x5 grid\n",
    "ships = {}\n",
    "ships['cruiser'] = 3\n",
    "\n",
    "grid_size = 5\n",
    "num_timesteps = 100000 # this is number of moves and not number of episodes\n",
    "\n",
    "best_mean_reward, n_steps, step_interval, episode_interval = -np.inf, 0, 10000, 10000\n",
    "\n",
    "# Instantiate the env\n",
    "env = BattleshipEnv(enemy_board=None, ship_locs={}, grid_size=grid_size, ships=ships)\n",
    "\n",
    "# wrap it\n",
    "log_dir = \"./gym/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "env = Monitor(env, filename=log_dir, allow_early_resets=True)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Train the agent - Note: best model is not save in Callback function for PPO2; save manually\n",
    "model = A2C('MlpPolicy', env, verbose=0).learn(total_timesteps=num_timesteps, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c598ab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(log_dir, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecef02d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best = A2C.load('./gym/best_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276a0595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "ships = {}\n",
    "ships['cruiser'] = 3\n",
    "\n",
    "grid_size=7\n",
    "enemy_board = 0*np.ones((grid_size, grid_size), dtype='int')\n",
    "#enemy_board[3,5] = 1\n",
    "#enemy_board[4,5] = 1\n",
    "#enemy_board[5,5] = 1\n",
    "env = BattleshipEnv(enemy_board=None, ship_locs={}, grid_size=grid_size, ships=ships)\n",
    "# give me time to setup recording\n",
    "time.sleep(5)\n",
    "for ep in range(10):\n",
    "    obs = env.reset()\n",
    "    ## 2 empty boards\n",
    "    done = False\n",
    "    nmoves = 0\n",
    "    print('episode no.', ep, '# moves:', nmoves)\n",
    "    env.render()\n",
    "    env.render()\n",
    "    time.sleep(5)\n",
    "    clear_output(wait=True)        \n",
    "    while not done:\n",
    "        action, obs = model_best.predict(obs, deterministic=True)\n",
    "        obs, _, done , _ = env.step(action)\n",
    "        nmoves += 1\n",
    "        print('episode no.', ep, '# moves:', nmoves)\n",
    "        env.render()\n",
    "        board_rendering(grid_size, env.enemy_board)\n",
    "        time.sleep(np.random.uniform(1,3))\n",
    "        clear_output(wait=True)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab5773",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
